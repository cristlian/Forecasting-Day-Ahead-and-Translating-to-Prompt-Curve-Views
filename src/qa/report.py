"""QA report generation."""

import json
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Dict

from .checks import CheckResult

logger = logging.getLogger(__name__)


def generate_qa_report(
    results: List[CheckResult],
    output_dir: Path,
    run_id: str,
    timestamp: datetime = None
) -> tuple[Path, Path]:
    """
    Generate QA report in JSON and Markdown formats.
    
    Args:
        results: List of CheckResult objects
        output_dir: Directory to save reports
        run_id: Unique run identifier
        timestamp: Timestamp for report
        
    Returns:
        Tuple of (json_path, md_path)
    """
    if timestamp is None:
        timestamp = datetime.now(timezone.utc)
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate JSON report
    json_path = output_dir / f"{run_id}_qa.json"
    _write_json_report(results, json_path, timestamp, run_id)
    
    # Generate Markdown report
    md_path = output_dir / f"{run_id}_qa.md"
    _write_md_report(results, md_path, timestamp, run_id)
    
    logger.info(f"QA reports saved to {output_dir}")
    
    return json_path, md_path


def _write_json_report(
    results: List[CheckResult],
    filepath: Path,
    timestamp: datetime,
    run_id: str
) -> None:
    """Write QA report as JSON."""
    report = {
        "run_id": run_id,
        "timestamp": timestamp.isoformat(),
        "overall_pass": all(r.passed for r in results),
        "summary": {
            "total_checks": len(results),
            "passed": sum(1 for r in results if r.passed),
            "failed": sum(1 for r in results if not r.passed),
            "total_errors": sum(len(r.errors) for r in results),
            "total_warnings": sum(len(r.warnings) for r in results),
        },
        "checks": [
            {
                "name": r.name,
                "passed": r.passed,
                "errors": r.errors,
                "warnings": r.warnings,
                "metrics": r.metrics,
            }
            for r in results
        ],
    }
    
    with open(filepath, 'w') as f:
        json.dump(report, f, indent=2, default=str)


def _write_md_report(
    results: List[CheckResult],
    filepath: Path,
    timestamp: datetime,
    run_id: str
) -> None:
    """Write QA report as Markdown."""
    overall_pass = all(r.passed for r in results)
    
    lines = [
        f"# QA Report: {run_id}",
        "",
        f"**Generated:** {timestamp.strftime('%Y-%m-%d %H:%M:%S')} UTC",
        "",
        "---",
        "",
        "## Summary",
        "",
    ]
    
    # Overall status banner
    if overall_pass:
        lines.append("### ✅ QA GATE: PASSED")
    else:
        lines.append("### ❌ QA GATE: FAILED")
    
    lines.append("")
    
    # Summary table
    lines.append("| Check | Status | Errors | Warnings |")
    lines.append("|-------|:------:|:------:|:--------:|")
    
    for r in results:
        status = "✅ PASS" if r.passed else "❌ FAIL"
        lines.append(f"| {r.name} | {status} | {len(r.errors)} | {len(r.warnings)} |")
    
    lines.append("")
    
    # Statistics
    total = len(results)
    passed = sum(1 for r in results if r.passed)
    lines.append(f"**Total Checks:** {total} | **Passed:** {passed} | **Failed:** {total - passed}")
    lines.append("")
    
    # Detailed results
    lines.append("---")
    lines.append("")
    lines.append("## Detailed Results")
    lines.append("")
    
    for r in results:
        status_icon = "✅" if r.passed else "❌"
        lines.append(f"### {status_icon} {r.name}")
        lines.append("")
        
        # Errors
        if r.errors:
            lines.append("**Errors:**")
            for error in r.errors:
                lines.append(f"- ❌ {error}")
            lines.append("")
        
        # Warnings
        if r.warnings:
            lines.append("**Warnings:**")
            for warning in r.warnings:
                lines.append(f"- ⚠️ {warning}")
            lines.append("")
        
        # Metrics
        if r.metrics:
            lines.append("**Metrics:**")
            lines.append("")
            lines.append("| Metric | Value |")
            lines.append("|--------|-------|")
            for key, value in r.metrics.items():
                if isinstance(value, float):
                    lines.append(f"| {key} | {value:.4f} |")
                else:
                    lines.append(f"| {key} | {value} |")
            lines.append("")
        
        if not r.errors and not r.warnings and not r.metrics:
            lines.append("*No issues detected.*")
            lines.append("")
    
    # Footer
    lines.append("---")
    lines.append("")
    lines.append("*Report generated by power-fair-value QA pipeline*")
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))


def format_qa_summary_for_log(results: List[CheckResult]) -> str:
    """Format QA results for logging."""
    lines = ["QA Summary:"]
    
    for r in results:
        status = "PASS" if r.passed else "FAIL"
        lines.append(f"  [{status}] {r.name}")
        
        if r.errors:
            for error in r.errors[:3]:  # Limit to first 3 errors
                lines.append(f"         ❌ {error}")
            if len(r.errors) > 3:
                lines.append(f"         ... and {len(r.errors) - 3} more errors")
    
    overall = "PASSED" if all(r.passed for r in results) else "FAILED"
    lines.append(f"  Overall: {overall}")
    
    return '\n'.join(lines)
